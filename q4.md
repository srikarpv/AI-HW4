List all algorithms and methods that we have covered in this course. Write 3 sentences to describe what each algorithm and method solves etc.


* **Breadth First Search:** This algorithm works to find the shortest path to an item, location, or goal state. It does so my searching breadth wise, exploring all neighbor nodes before searching deeper. We used this in our Pacman AI algorithm for eating all the pellets in a maze. Breadth first search starts at the root node and it explores all the neighbor nodes to the current node before moving to the next depth level.


* **Depth First Search:** This algorithm works to find the shortest path to an item. However, unlike DFS, it search as deep as it can first before searching neighbor nodes. We used this in our Pacman AI algorithm for eating all the pellets in a maze. Depth first search starts at the root node and it explores as far as possible along each branch before backtracking.



* **Dijkstra's Algorithm:** Dijkstra's algorithm is to find the shortest path between a and b. It picks the unvisited vertex with the lowest distance, calculates the distance through it to each unvisited neighbor, and updates the neighbor's distance if smaller.
* Dijkstras algorithm finds the shortest paths between nodes in a graph. Start with an initial node and then visit every node updating the distance from the initial node to that node every time a shorter path is found.



* **Uniform Cost Search:** This algorithm  works to find the shortest path to an item. The algorithm starts at the root node, and it explores nodes in order of the minimum cost back to the root. In other words, it explores the minimum cost vertices to the root vertex. UCS is complete and optimal, but it explores options in every “direction” and has no information about the goal location. It is very similar to Dijsktra's algorthm and searches using a priority queue. This priority queue is used to decide which path to take, without the use of heuristics.


* **Minimum spanning trees(Kruskal's algorithm, greedy algorithm,Prim's algorithm):** An edge-weighted graph is a graph where we associate weights or costs with each edge. A minimum spanning tree (MST) of an edge-weighted graph is a spanning tree whose weight (the sum of the weights of its edges) is no larger than the weight of any other spanning tree.
* Greedy algorithm finds the minimum spanning tree by always choosing the minimum weight to connect vertices
* Kruskal algorithm finds a MST by choosing the minimum weight edge in a graph as long as: there are edges to choose from, the edge doesnt form a cycle, and we still haven't found a MST
* Prim's algorithm finds a MST by randomly choosing a vertex to begin building the MST, and then iteratively add the minimum edge that connects a vertex not yet in the tree to a vertex in the tree


* **A\*Search:** This algorithm works to find the shortest path to an item. However, unlike the algorithms above, Auses a heuristic, along with the actual path cost, to determine which path to take. The heuristic used should ideally be both admissable and consistent. Admissable being that it doesn't overestimate the goal cost, and consistent being that the hueristic can at most drop a cost c when taking an action, where c is the cost of the taken action.


* **Minimax:** Minimax is used to decide the actions to take to best maximize the reward and minimize the potential loss. The algorithm works by alternating agency between the player and the opponent. The values are calculated for the worst possible scenario, where the opponent makes the best moves possible.

* **Priority Queues:** They are similar to queue where we insert an element from the back and remove an element from front, but with a one difference that the logical order of elements in the priority queue depends on the priority of the elements. The element with highest priority will be moved to the front of the queue and one with lowest priority will move to the back of the queue.

* **Alpha-Beta Pruning:** This algorithm is used to prune the results given my Minimax, to make it more efficient. Instead of continiuing to evaluate a minimax tree branch, it will stop whenever a better action is found, as these lesser branches will not affect the final decision. 
Alpha Beta pruning seeks to decrease the number of nodes that are evaluated by the minimax algorithm in its search tree. It basically avoids expanding nodes that wont be played because they will be ignored by the min/max agent(s).
* It uses two values, alpha and beta, which represent the minumum score for the player and the maximum score for the opponent respectively. A branch is not explored whenever beta is less than or equal to alpha, as these actions will not be considered for the final result.


* **Expectimax:** Expectimax is similar to Minimax, except that it no longer assumes we are playing against a perfect opponent. Instead it uses probabilistic modeling to emulate opponent behavior, where they may not always choose the optimal action. This allows us to choose the best probability model for the opponent.
* Expectimax search is similiar to Minimax, but values should now reflect average-case (expectimax) outcomes, not worst-case (minimax) outcomes. On a expectimax search, we aim to compute the average score under optimal play. The max nodes remain the same as in minimax, but the min nodes are now "chance" nodes with a calculated weighted average of its children. This weighted average is tied to the probability of the children node happening. In expectimax, we have a probabilistic model of how opponents will behave at a given state.


* **Value Iteration:** Value iteration is a planner which computes a Markov Decision Process (MDP) policy through backwards induction. This optimal computed policy is derived through an iterative process that runs until convergence.


* **Q-Learning:** This algorithm is used to find a policy that will indicate which action to take under different circumstances. It takes into account the old Q value, the learning rate, and the learned value which is composed of the reward, discount factor, and estimate of the next optimal Q value. The end result allows an agent to take the best possible action given a specific state.
* Q-learning is a reinforcement learning technique used in machine learning. The goal of Q-learning is to learn a policy, which tells an agent what action to take under what circumstances. It does not require a model of the environment and can handle problems with stochastic transitions and rewards, without requiring adaptations" (Wikipedia (2018-c)). Can be implemented by using the methods update, computeValueFromQValues, getQValue, and computeActionFromQValues methods: Updating the Q-Value by implementing the Q-Learning Algorythm Q(s,a) = (1-alpha) * Q(s,a) + alpha * (reward + discount * max_a' (Q(s',a')). Computing Values from QValues by returning the max_action Q(state,action).


* **Cross-Entropy Method:** This method is used to produce better samples given a distribution that is different from the one we are interested in. It works by reducing the amount of cross entropy, which is used as a quantitave measure of the difference between two distributions, from the given distribution and the target distribition. This can be particularly helful for simulating rare-events.